{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f831b730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\MyData\\yolov5_data\\yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd yolov5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cb2e8f",
   "metadata": {},
   "source": [
    "# For Yolov5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "470a6ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from numpy import random\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import (non_max_suppression, scale_coords, strip_optimizer, check_img_size,xyxy2xywh)\n",
    "from utils.torch_utils import select_device\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from pystackreg import StackReg\n",
    "\n",
    "from uuid import uuid4\n",
    "import requests\n",
    "\n",
    "import shutil\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040a403a",
   "metadata": {},
   "source": [
    "#  For sahi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21f7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required functions, classes\n",
    "from sahi.model import Yolov5DetectionModel\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from IPython.display import Image\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6aa11",
   "metadata": {},
   "source": [
    "#  Functions Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba2521dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "def init():\n",
    "    global model\n",
    "    global names\n",
    "    global imgsz\n",
    "    global device\n",
    "    imgsz = 640\n",
    "    device = select_device(\"\")\n",
    "    # Load model\n",
    "    model = attempt_load(\"runs/train/exp12/weights/best.pt\", map_location=device)  # load FP32 model\n",
    "    imgsz = check_img_size(imgsz, s=model.stride.max())  # check img_size\n",
    "\n",
    "\n",
    "    names = model.module.names if hasattr(model, 'module') else model.names\n",
    "\n",
    "# get fdetections\n",
    "def detect_image(img_main):\n",
    "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)\n",
    "\n",
    "    img = letterbox(img_main, new_shape=imgsz)[0]\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "\n",
    "    img = img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # pred\n",
    "    pred = model(img, augment=True)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, 0.4, 0.2, classes=None, agnostic=False)\n",
    "\n",
    "\n",
    "    results = []\n",
    "    # Process detections\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        if det is not None and len(det):\n",
    "            # Rescale boxes from img_size to img_main size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img_main.shape).round()\n",
    "            \n",
    "            \n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                results.append({\n",
    "                    'xmin' : int(xyxy[0]), \n",
    "                    'ymin' : int(xyxy[1]),\n",
    "                    'xmax' : int(xyxy[2]), \n",
    "                    'ymax' : int(xyxy[3]),\n",
    "                    'class' : names[int(cls)]\n",
    "                })\n",
    "    return results\n",
    "\n",
    "# get b_boxes\n",
    "def get_b_boxes(img_main):\n",
    "    img = torch.zeros((1, 3, imgsz, imgsz), device=device)\n",
    "\n",
    "    img = letterbox(img_main, new_shape=imgsz)[0]\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "\n",
    "    img = img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # pred\n",
    "    pred = model(img, augment=True)[0]\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, 0.3, 0.3, classes=None, agnostic=False)\n",
    "\n",
    "\n",
    "    Boxes = []\n",
    "    # Process detections\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        if det is not None and len(det):\n",
    "            # Rescale boxes from img_size to img_main size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], img_main.shape).round()\n",
    "            \n",
    "            \n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                Boxes.append([int(xyxy[0]), int(xyxy[1]),int(xyxy[2]),int(xyxy[3])])\n",
    "    return Boxes\n",
    "\n",
    "def compute_iou(bounding_box_1, bounding_box_2):\n",
    "    \"\"\"\n",
    "    This function computes the Intersection over Union of two bounding boxes.\n",
    "    :param bounding_box_1: bounding box coordinates of 1st box\n",
    "    :param bounding_box_2:\n",
    "    :return: computed IoU\n",
    "    \"\"\"\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "    xA = max(bounding_box_1[0], bounding_box_2[0])\n",
    "    yA = max(bounding_box_1[1], bounding_box_2[1])\n",
    "    xB = min(bounding_box_1[2], bounding_box_2[2])\n",
    "    yB = min(bounding_box_1[3], bounding_box_2[3])\n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    # compute the area of both the prediction and ground-trut rectangles\n",
    "    boxAArea = (bounding_box_1[2] - bounding_box_1[0] + 1) * (bounding_box_1[3] - bounding_box_1[1] + 1)\n",
    "    boxBArea = (bounding_box_2[2] - bounding_box_2[0] + 1) * (bounding_box_2[3] - bounding_box_2[1] + 1)\n",
    "\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def resize_with_padding(img,new_image_width,new_image_height):\n",
    "    old_image_height, old_image_width, channels = img.shape\n",
    "\n",
    "    # create new image of desired size and color (blue) for padding\n",
    "    color = (0,0,0)\n",
    "    \n",
    "    result = np.full((new_image_height,new_image_width, channels), color, dtype=np.uint8)\n",
    "\n",
    "    # compute center offset\n",
    "    x_center = (new_image_width - old_image_width) // 2\n",
    "    y_center = (new_image_height - old_image_height) // 2\n",
    "\n",
    "    # copy img image into center of result image\n",
    "    result[y_center:y_center+old_image_height, \n",
    "           x_center:x_center+old_image_width] = img\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "sr = StackReg(StackReg.RIGID_BODY)\n",
    "class ImageRegister():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def register_images(self, base_image, images_list):\n",
    "        \"\"\" Register images using pystackreg \"\"\"\n",
    "        \n",
    "        # Read base image, get shape and remove from directory\n",
    "        base_image = cv2.imread(base_image)\n",
    "        base_shape = base_image.shape\n",
    "        \n",
    "\n",
    "        for image_path in images_list: # Iterate over images (ref) and register each image againt base image\n",
    "\n",
    "            # read ref image and resize to base image size\n",
    "            mov_image = cv2.resize(cv2.imread(image_path, -1), (base_shape[1], base_shape[0]))\n",
    "\n",
    "            # Register image\n",
    "            sr.register(base_image[:,:,0], mov_image[:,:,0])\n",
    "            out_aff_0 = sr.transform(mov_image[:, :, 0])\n",
    "            out_aff_1 = sr.transform(mov_image[:,:,1])\n",
    "            out_aff_2 = sr.transform(mov_image[:,:,2])\n",
    "\n",
    "            ref_shape=mov_image.shape\n",
    "            out_aff = np.zeros(ref_shape)\n",
    "            out_aff[:,:,0] = out_aff_0\n",
    "            out_aff[:,:,1] = out_aff_1\n",
    "            out_aff[:,:,2] = out_aff_2\n",
    "\n",
    "            #stacked_image = np.hstack((base_image, cv2.normalize(src=out_aff, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)))\n",
    "            \n",
    "            \n",
    "            # save registered image to disk\n",
    "            ##stacked_image_name = f\"{uuid4()}.jpg\"           \n",
    "            #cv2.imwrite(\"Testing_Changes/1C.jpg\", cv2.normalize(src=out_aff, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U))\n",
    "        \n",
    "        return cv2.normalize(src=out_aff, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6aafa2",
   "metadata": {},
   "source": [
    "#  Loading Model for Detetection by Slicing ( Sahi )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aea8f33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      8800  yolov5.models.common.Conv               [3, 80, 6, 2, 2]              \n",
      "  1                -1  1    115520  yolov5.models.common.Conv               [80, 160, 3, 2]               \n",
      "  2                -1  4    309120  yolov5.models.common.C3                 [160, 160, 4]                 \n",
      "  3                -1  1    461440  yolov5.models.common.Conv               [160, 320, 3, 2]              \n",
      "  4                -1  8   2259200  yolov5.models.common.C3                 [320, 320, 8]                 \n",
      "  5                -1  1   1844480  yolov5.models.common.Conv               [320, 640, 3, 2]              \n",
      "  6                -1 12  13125120  yolov5.models.common.C3                 [640, 640, 12]                \n",
      "  7                -1  1   7375360  yolov5.models.common.Conv               [640, 1280, 3, 2]             \n",
      "  8                -1  4  19676160  yolov5.models.common.C3                 [1280, 1280, 4]               \n",
      "  9                -1  1   4099840  yolov5.models.common.SPPF               [1280, 1280, 5]               \n",
      " 10                -1  1    820480  yolov5.models.common.Conv               [1280, 640, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 13                -1  4   5332480  yolov5.models.common.C3                 [1280, 640, 4, False]         \n",
      " 14                -1  1    205440  yolov5.models.common.Conv               [640, 320, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 17                -1  4   1335040  yolov5.models.common.C3                 [640, 320, 4, False]          \n",
      " 18                -1  1    922240  yolov5.models.common.Conv               [320, 320, 3, 2]              \n",
      " 19          [-1, 14]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 20                -1  4   4922880  yolov5.models.common.C3                 [640, 640, 4, False]          \n",
      " 21                -1  1   3687680  yolov5.models.common.Conv               [640, 640, 3, 2]              \n",
      " 22          [-1, 10]  1         0  yolov5.models.common.Concat             [1]                           \n",
      " 23                -1  4  19676160  yolov5.models.common.C3                 [1280, 1280, 4, False]        \n",
      " 24      [17, 20, 23]  1     40374  yolov5.models.yolo.Detect               [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
      "c:\\users\\dsp-24\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Model Summary: 567 layers, 86217814 parameters, 86217814 gradients, 204.2 GFLOPs\n",
      "\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "yolov5_model_path = 'D:/MyData/yolov5_data/yolov5/runs/train/exp12/weights/best.pt'\n",
    "detection_model = Yolov5DetectionModel(\n",
    "    model_path=yolov5_model_path ,\n",
    "    confidence_threshold=0.3,\n",
    "    device=\"cuda:0\", \n",
    "    image_size=640\n",
    "    #load_at_init=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466966ac",
   "metadata": {},
   "source": [
    "# Change Detection By Detections Using Slicing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb3734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Checking changes in Building\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load Model\n",
    "\n",
    "\n",
    "A='D:\\MyData\\yolov5_data\\sahi\\Test2_A.jpg'\n",
    "B='D:\\MyData\\yolov5_data\\sahi\\Test2_B.jpg'\n",
    "C='D:\\MyData\\yolov5_data\\sahi\\Test2_C.jpg'\n",
    "\n",
    "\n",
    "# read image using opnecv\n",
    "After = cv2.imread(A)\n",
    "\n",
    "# registering before image using after as a refrence\n",
    "IR=ImageRegister()\n",
    "Before=IR.register_images(A,[B])\n",
    "cv2.imwrite(B,Before)\n",
    "\n",
    "#Before = cv2.imread(\"Testing_Changes/4B.jpg\")\n",
    "\n",
    "Results_A=get_sliced_prediction(\n",
    "    A,\n",
    "    detection_model,\n",
    "    slice_height =640,\n",
    "    slice_width = 640,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2,\n",
    "     perform_standard_pred=False,\n",
    "     postprocess_type=\"NMS\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "    postprocess_match_threshold=0.4\n",
    ")\n",
    "\n",
    "Boxes_A = [] # pass image \n",
    "for prediction in Results_A.object_prediction_list:\n",
    "    Boxes_A.append(prediction.bbox.to_voc_bbox())\n",
    "\n",
    "for box in Boxes_A:\n",
    "    cv2.rectangle(After, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 2) # draw rectangle\n",
    "\n",
    "\n",
    "\n",
    "Results_B=get_sliced_prediction(\n",
    "    B,\n",
    "    detection_model,\n",
    "    slice_height =640,\n",
    "    slice_width = 640,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2,\n",
    "     perform_standard_pred=False,\n",
    "     postprocess_type=\"NMS\",\n",
    "    postprocess_match_metric=\"IOS\",\n",
    "    postprocess_match_threshold=0.4\n",
    ")\n",
    "\n",
    "Boxes_B = [] # pass image\n",
    "for prediction in Results_B.object_prediction_list:\n",
    "    Boxes_B.append(prediction.bbox.to_voc_bbox())\n",
    "\n",
    "for box in Boxes_B:\n",
    "    cv2.rectangle(Before, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 2) # draw rectangle\n",
    "\n",
    "\n",
    "\n",
    "## Detecting Changes\n",
    "\n",
    "Change = cv2.imread(A)\n",
    "cv2.imwrite('D:\\MyData\\yolov5_data\\sahi\\Test2_A_R.jpg',After)\n",
    "cv2.imwrite('D:\\MyData\\yolov5_data\\sahi\\Test2_B_R.jpg',Before)\n",
    "\n",
    "for box in Boxes_A:\n",
    "    c=0\n",
    "    for box1 in Boxes_B:\n",
    "        if(compute_iou(box,box1))<0.4:\n",
    "            c+=1\n",
    "    if c==len(Boxes_B):\n",
    "        cv2.rectangle(Change, (box[0], box[1]), (box[2], box[3]), (0, 0, 255), 2) # draw rectangle\n",
    "        cv2.putText(Change, 'Changed', (box[0],box[1]), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 5)\n",
    "        \n",
    "     \n",
    "cv2.imwrite(C,Change)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c05a15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
